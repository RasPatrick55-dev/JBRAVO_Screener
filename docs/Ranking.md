# Ranking Evaluation and Autotuning

This document summarises the nightly artefacts generated by the screener and
how they are consumed by the ranker evaluation and autotuning utilities.

## Nightly predictions snapshot

Running `scripts/screener.py` now emits a daily snapshot at
`data/predictions/YYYY-MM-DD.csv` alongside the existing outputs. The file
contains one row per ranked symbol (including those that failed gate checks)
with the following fields:

| Column            | Description |
| ----------------- | ----------- |
| `timestamp`       | Timestamp of the bars row used for scoring (UTC). |
| `symbol`          | Uppercase symbol. |
| `Score`           | Composite ranker score using the configured weights. |
| `rank`            | One-based ordinal ranking (1 = best). |
| `gates_passed`    | Boolean indicating whether the nightly gate checks passed. |
| feature columns   | Raw feature values exported by the screener (RSI, ADX, SMA9, etc.). |
| `score_breakdown` | JSON blob containing the z-scored component values used for weighting. |

The snapshot is the canonical source for downstream evaluation and tuning. It
contains every feature required to reproduce the nightly scores, plus gate
context for filtering experiments.

## Label computation

`scripts/labels/make_nextday_labels.py` joins the nightly prediction snapshot
with realized forward performance. Labels are derived using daily OHLC data
sourced from `data/daily_prices.csv` (or an alternative path passed via
`--prices`). For each prediction row:

1. Locate the matching close on the prediction date.
2. Scan the next *H* trading days (defaults to `--label-horizon 3`).
3. If any day's high reaches at least `hit_threshold` (defaults to 0.04 = 4%)
   before a drawdown of the same magnitude occurs, mark the row as a hit.
4. If a drawdown of `drawdown_threshold` (defaults to the hit threshold) occurs
   first, or the horizon expires without a hit, mark the row as a miss.

Running `scripts/ranker_eval.py` over the rolling history of labels produces a
compact nightly summary at `data/ranker_eval/summary.json` alongside a decile
breakdown stored as `data/ranker_eval/deciles_YYYY-MM-DD.csv`. A `latest.json`
symlink mirrors the most recent summary for dashboard consumers. The JSON
payload includes:

- metadata fields (`generated_at_utc`, `window_days`, evaluated population)
- a `metrics` object covering hit rate, average expectancy, profit factor,
  Sharpe, maximum drawdown, and the standard deviation of realised returns
- calibration bins (`calibration`) and monthly stability aggregates
  (`stability`) for quick sanity checks

All numeric metrics are serialised as native floats (or `null` when not
available) and the decile CSV is re-exported as `latest_deciles.csv` for simple
embedding inside dashboards.

## Autotuning workflow

`scripts/ranker_autotune.py` consumes the labelled dataset to propose updated
component weights via walk-forward cross validation. The process is:

1. Load the latest `configs/ranker_v2.yml` weights and feature order.
2. Collect realised labels for the requested lookback window and standardise the
   component columns.
3. Split the history into chronological folds (default: four walk-forward
   splits) and score each test fold with the candidate weights.
4. Evaluate the top quantile of scores for expectancy, profit factor, and
   drawdown while tracking per-fold expectancy variance.
5. Apply guardrails: profit factor must exceed 1.2, maximum drawdown must not
   breach the configured cap, and fold-level expectancy variance must remain
   below the supplied limit. Candidates must also beat the incumbent by at
   least `--min-improvement` (defaults to 10 bps/day).

Accepted proposals are versioned under `configs/ranker_v2_YYYYMMDD.yml`. The
script updates the canonical `configs/ranker_v2.yml` symlink atomically and
appends a Markdown entry to `configs/ranker_v2_changelog.md` summarising the
baseline versus candidate metrics. The JSON sidecar
(`configs/ranker_v2_YYYYMMDD.json`) records the aggregated expectancy,
guardrail checks, improvement margin, and the evaluated fold statistics for
auditability.

No files outside `configs/` are touched—human review can continue to manage the
primary `config/ranker.yml` manually when desired.

## Usage summary

```bash
# Derive next-day labels for the 2025-01-15 predictions snapshot
python -m scripts.labels.make_nextday_labels data/predictions/2025-01-15.csv \
    --prices data/daily_prices.csv

# Evaluate the trailing 90 labelled days and write ranker artefacts
python -m scripts.ranker_eval --days 90 --labels-dir data/labels --output-dir ranker_eval

# Run the guarded walk-forward autotuner using the last 18 weeks of labels
python -m scripts.ranker_autotune --lookback-days 126 --splits 5
```

Both commands write their artefacts atomically so downstream jobs always observe
complete files. Refer to the CLI `--help` output for advanced options such as
custom price sources or filtering to gate-passing rows only.
