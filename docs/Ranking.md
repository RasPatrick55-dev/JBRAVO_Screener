# Ranking Evaluation and Autotuning

This document summarises the nightly artefacts generated by the screener and
how they are consumed by the ranker evaluation and autotuning utilities.

## Nightly predictions snapshot

Running `scripts/screener.py` now emits a daily snapshot at
`data/predictions/YYYY-MM-DD.csv` alongside the existing outputs. The file
contains one row per ranked symbol (including those that failed gate checks)
with the following fields:

| Column            | Description |
| ----------------- | ----------- |
| `timestamp`       | Timestamp of the bars row used for scoring (UTC). |
| `symbol`          | Uppercase symbol. |
| `Score`           | Composite ranker score using the configured weights. |
| `rank`            | One-based ordinal ranking (1 = best). |
| `gates_passed`    | Boolean indicating whether the nightly gate checks passed. |
| feature columns   | Raw feature values exported by the screener (RSI, ADX, SMA9, etc.). |
| `score_breakdown` | JSON blob containing the z-scored component values used for weighting. |

The snapshot is the canonical source for downstream evaluation and tuning. It
contains every feature required to reproduce the nightly scores, plus gate
context for filtering experiments.

## Label computation

`scripts/eval_ranker.py` transforms the rolling window of prediction snapshots
into binary classification labels based on forward returns. Labels are derived
using daily OHLC data sourced from `data/daily_prices.csv` (or an alternative
path passed via `--prices`). For each prediction row:

1. Locate the matching close on the prediction date.
2. Scan the next *H* trading days (defaults to `--label-horizon 3`).
3. If any day's high reaches at least `hit_threshold` (defaults to 0.04 = 4%)
   before a drawdown of the same magnitude occurs, mark the row as a hit.
4. If a drawdown of `drawdown_threshold` (defaults to the hit threshold) occurs
   first, or the horizon expires without a hit, mark the row as a miss.

The evaluator emits `data/ranker_eval/YYYY-MM-DD.json` containing a compact
summary with:

- metadata fields (`generated_at_utc`, `window_days`, `label_horizon_days`,
  thresholds, evaluated population)
- a `metrics` object with ROC AUC (`auc`), precision-recall AUC (`pr_auc`),
  and optional extras such as `gate_pass_rate`
- a `deciles` object containing parallel arrays for `rank_decile`,
  per-decile `hit_rate`, average forward return (`avg_return`), and sample
  counts (`count`)

All numeric metrics are serialised as native floats (or `null` when not
available) to simplify dashboarding.

## Autotuning workflow

`scripts/autotune_ranker.py` consumes the labelled dataset to propose updated
component weights. The process is:

1. Load the latest `config/ranker.yml` weights and component order.
2. Build a training matrix from the `score_breakdown` z-scores exported in the
   nightly snapshots.
3. Split the history by date into training and validation windows (most recent
   20% of days by default).
4. Fit an L2-regularised logistic regression on the training portion.
5. Rescale the fitted coefficients to the same magnitude as the current weights
   and constrain each component to Â±20% of its existing value. Trend-oriented
   components (`trend`, `momentum`, `breakout`, `pivot_trend`, `multi_horizon`)
   keep their original sign.
6. Accept the proposal only if the validation ROC AUC improves by at least
   `--delta` (default 0.01) **and** the validation sample contains at least
   `--min-sample` rows.

Results are persisted to `data/model_state.json`. The payload includes:

- baseline vs candidate AUC, absolute improvement, thresholds used
- train/validation sample sizes and date ranges
- current weights, raw logistic fit, and constrained preview weights
- the logistic model bias and raw coefficients (for auditability)
- a flag indicating whether the update met the acceptance criteria

The autotuner never edits `config/ranker.yml`; operations are limited to the
preview JSON so human review can decide whether to apply changes.

## Usage summary

```bash
# Evaluate the last 90 prediction files with a 4% hit threshold
python -m scripts.eval_ranker --days 90 --label-horizon 3 --hit-threshold 0.04 \
    --predictions-dir data/predictions --prices data/daily_prices.csv

# Propose new weights using the last 120 days, requiring +0.015 AUC uplift
python -m scripts.autotune_ranker --train-days 120 --delta 0.015 \
    --state-path data/model_state.json
```

Both commands write their artefacts atomically so downstream jobs always observe
complete files. Refer to the CLI `--help` output for advanced options such as
custom price sources or filtering to gate-passing rows only.
