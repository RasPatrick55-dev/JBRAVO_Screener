# Ranking Evaluation and Autotuning

This document summarises the nightly artefacts generated by the screener and
how they are consumed by the ranker evaluation and autotuning utilities.

## Nightly predictions snapshot

Running `scripts/screener.py` now emits a daily snapshot at
`data/predictions/YYYY-MM-DD.csv` alongside the existing outputs. The file
contains one row per ranked symbol (including those that failed gate checks)
with the following fields:

| Column            | Description |
| ----------------- | ----------- |
| `timestamp`       | Timestamp of the bars row used for scoring (UTC). |
| `symbol`          | Uppercase symbol. |
| `Score`           | Composite ranker score using the configured weights. |
| `rank`            | One-based ordinal ranking (1 = best). |
| `gates_passed`    | Boolean indicating whether the nightly gate checks passed. |
| feature columns   | Raw feature values exported by the screener (RSI, ADX, SMA9, etc.). |
| `score_breakdown` | JSON blob containing the z-scored component values used for weighting. |

The snapshot is the canonical source for downstream evaluation and tuning. It
contains every feature required to reproduce the nightly scores, plus gate
context for filtering experiments.

## Label computation

`scripts/labels/make_nextday_labels.py` joins the nightly prediction snapshot
with realized forward performance. Labels are derived using daily OHLC data
sourced from `data/daily_prices.csv` (or an alternative path passed via
`--prices`). For each prediction row:

1. Locate the matching close on the prediction date.
2. Scan the next *H* trading days (defaults to `--label-horizon 3`).
3. If any day's high reaches at least `hit_threshold` (defaults to 0.04 = 4%)
   before a drawdown of the same magnitude occurs, mark the row as a hit.
4. If a drawdown of `drawdown_threshold` (defaults to the hit threshold) occurs
   first, or the horizon expires without a hit, mark the row as a miss.

Running `scripts/ranker_eval.py` over the rolling history of labels produces a
compact nightly summary at `ranker_eval/summary.json` alongside a decile
breakdown stored as `ranker_eval/deciles_YYYY-MM-DD.csv`. The JSON payload
includes:

- metadata fields (`generated_at_utc`, `window_days`, `label_horizon_days`,
  thresholds, evaluated population)
- a `metrics` object with ROC AUC (`auc`), precision-recall AUC (`pr_auc`),
  and optional extras such as `gate_pass_rate`
- a `deciles` object containing parallel arrays for `rank_decile`,
  per-decile `hit_rate`, average forward return (`avg_return`), and sample
  counts (`count`)

All numeric metrics are serialised as native floats (or `null` when not
available) to simplify dashboarding.

## Autotuning workflow

`scripts/ranker_autotune.py` consumes the labelled dataset to propose updated
component weights. The process is:

1. Load the latest `config/ranker.yml` weights and component order.
2. Build a training matrix from the `score_breakdown` z-scores exported in the
   nightly snapshots.
3. Split the history by date into training and validation windows (most recent
   20% of days by default).
4. Fit an L2-regularised logistic regression on the training portion.
5. Rescale the fitted coefficients to the same magnitude as the current weights
   and constrain each component to Â±20% of its existing value. Trend-oriented
   components (`trend`, `momentum`, `breakout`, `pivot_trend`, `multi_horizon`)
   keep their original sign.
6. Accept the proposal only if the validation ROC AUC improves by at least
   `--delta` (default 0.01) **and** the validation sample contains at least
   `--min-sample` rows.

Accepted proposals are versioned under `configs/ranker_v2_YYYYMMDD.yml` and the
latest approved file is exposed via the `configs/ranker_v2_current.yml`
symlink. Each accepted update appends an entry to
`configs/ranker_v2_changelog.md` describing the baseline vs candidate
performance. The JSON sidecar (`configs/ranker_v2_YYYYMMDD.json`) stores:

- baseline vs candidate AUC, absolute improvement, thresholds used
- train/validation sample sizes and date ranges
- current weights, raw logistic fit, and constrained preview weights
- the logistic model bias and raw coefficients (for auditability)
- a flag indicating whether the update met the acceptance criteria

The autotuner never edits `config/ranker.yml`; operations are limited to the
preview JSON so human review can decide whether to apply changes.

## Usage summary

```bash
# Derive next-day labels for the 2025-01-15 predictions snapshot
python -m scripts.labels.make_nextday_labels data/predictions/2025-01-15.csv \
    --prices data/daily_prices.csv

# Evaluate the trailing 90 labelled days and write ranker artefacts
python -m scripts.ranker_eval --days 90 --labels-dir data/labels --output-dir ranker_eval

# Run the guarded walk-forward autotuner using the last 18 weeks of labels
python -m scripts.ranker_autotune --lookback-days 126 --splits 5
```

Both commands write their artefacts atomically so downstream jobs always observe
complete files. Refer to the CLI `--help` output for advanced options such as
custom price sources or filtering to gate-passing rows only.
